# Code Review
## General Info
### Metrics
The traditional metric for code review is lines of code. 
The number I most requently hear thrown around is 100KLOC per person-week when using static analysis tools. 
I go back and forth on whether or not this is a reasonable number. 
Static analysis tools do cut way down on the work you need to do, but they also have many false positives. 

When I'm scoping such a project and I'm not planning to rely solely on static analysis tools, I much prefer another metric: number of files. 
While this can fall down in the case of massive files (and I do check for this case ahead of time), 10 files per day for manual review is absolutely reasonable and doable. 

### Quality
The quality of source review rests on whether or not you're manually reviewing the whole app. 
Using static analysis tools is quick, but it won't reliably find many classes of bugs. 
When doing manual review, you're going to work at 1% of the speed, but you're going to have a much higher quality result. 

### Balance
But all of this is a balancing act. It's simply not viable to review huge apps manually, and automated scanning is cumbersome and prone to failure. 
So my recommendation is this: <100KLOC is manual review with augmentation from static analysis tools. 
The opposite is true of >100KLOC, with manual review for authentication, credential storage, crypto, and other things of that nature. 

### Communication
If you're doing this in a consulting environment, communication is absolutely critical with all of this, to ensure the client knowns what they are going to recieve. 
In most cases, you will have a good idea oof what you can actually accomplish given the timefram, so just communicate that to the client. 

## Static Analysis Tools
### Fortify
Fortify is one of the most well-known static analysis tools. 
What's really interesting is that it doesn't just look at your code, but rather actually compiles it. 
This allows extremely deep analysis of how functions tie together and can, for iinstance, find SQL injection that crosses multiple function boundaries. 
All of this means that it's a great product for certain cases, but its prices start at 5 figures USD and go rpidly up from there. This makes it impractical for many purposes. 
In addition, it's really only terribly useful for C, C++, C#, and Java

### Checkmarx
Checkmarx is a very nbew tool, but in my experience it's excellent for web apps.
It's primarily useful for Java and C#, but it's a fraction of the price of Fortify and does just as good a job. 
The interesting thing about this product is that it takesthe entire codebase and tyransforms it into, essentially, a queryable abstract syntax tree. This could let you do entrypoint anlysis and more, fine-tuned for your exact case. 
However, they typically restrict its queries to doing vulnerability scanning and make this kind of exercise difficult. I believe there's great potential here for the future. 

### Others
* Commercial
  * Coverity - Not as useful for web apsp
  * Veracode - Hosted service, but you give them binaries rather that source (usually). Expensive, but effective. 
* Open Source
  * OWASP Static Analysis tools - Largely unmaintained and need a lot of work, but focused on the web. 

### Failings
False positives are the big concern with static source analysis tools. you can get thousands of false positives from small code bases, making it easy to lose real bugs in the mix. 
Most of the time, I end up writing scripts to verify the contents quickly, as the false positives in a codebase will often fall into a simple pattern. 

### Component-based Splitting
When dividing a large review between multiple testers, the simplest route is to divide the entire codebase into its component parts. 
Each tester will take a number of components, which they test independently. 
Progress tracking is generally done by file counts, which can be decfeptive as many files are smaller than others. 
